# A template for Nextflow workflows
![](https://github.com/stracquadaniolab/cookiecutter-workflow-nf/workflows/build/badge.svg)

A standardized directory structure to build workflow using Nextflow. 

## Directory structure

```
|-- bin
|   `-- main.py
|-- conf
|   |-- base.config
|   |-- ci.config
|   |-- sge.config
|   |-- test.config
|   `-- workstation.config
|-- containers
|   `-- Dockerfile
|-- environment.yml
|-- main.nf
|-- nextflow.config
`-- testdata
```

## Overview

The `main.nf` file contains the entrypoint for the workflow, and it uses DSL2 by
default. The workflow configuration is stored in the `nextflow.config` file and
under the `conf` directory; usually, you only have to define the parameters of
your specific pipeline, since the other configurations are profiles to run the
pipeline in different computing environment, e.g. SGE, GitHub. Please, refer to
the [Nextflow documentation](https://www.nextflow.io/docs/latest/index.html) for
an overview of the framework.

### Custom scripts management

Custom code need by the pipeline should be added to the `bin` directory; the
code in this directory is automatically added to `$PATH` when running the
pipeline, which makes custom scripts easily portable and accessible. If you are
using Python, you should have a `main.py` file, which calls other commands
through `argh`. See the auto-generated pipeline for an example.

### Software management

Third-party software is managed by `conda` and specified in a `environment.yml`
file; keep the `yml` file updated and specify the version of each software you
are using.

To ensure reproducibility and facilitate running experiments between local
machine and HPC clusters, it is strongly recommended to build a Docker image.
The bundled `Dockerfile` can be used to build an image with the software
specified in your `environment.yml` file. To do that, run:

```bash
docker build . -t stracquadaniolab/my-pipeline -f containers/Dockerfile
```

where `my-pipeline` is the name of your workflow.

### Testing

It is important to build workflows that can be automatically tested; thus, you
will have to add small test data into the `testdata` directory, and modify the
`conf/test.config` configuration file to specify any parameter needed for your
workflow to run. See the auto-generated pipeline for an example.

### Documentation

Each workflow must have an updated `readme.md` file, describing:

* what the workflow does
* how to configure the workflow
* how to run the workflow
* a description of the output generated

A `readme.md` file with the required sections is automatically generated by this
cookiecutter.

### Continuous integration

Each pipeline comes with a pre-configured GitHub workflow to automatically test
the code and build a Docker image; the workflow is stored in
`.github/workflows/ci.yml`.

## Getting started

### Step 1: Create a new workflow

```
cookiecutter git@github.com:stracquadaniolab/cookiecutter-workflow-nf.git
```

You will be asked a number of questions to setup your workflow.

### Step 2: Push the pipeline to GitHub

```bash
git push -u origin master
```

### Step 3: Run a pipeline with test data

```bash
nextflow run stracquadaniolab/my-pipeline -profile test
```

When you run a pipeline for the first time, it will download the latest release
from GitHub and run it on the test data.

### Step 4: Run a pipeline with test data and Docker

```bash
nextflow run stracquadaniolab/my-pipeline -profile test,docker
```

### Step 5 (optional): Run a pipeline on a cluster with Sun Grid Engine

```bash
nextflow run stracquadaniolab/my-pipeline -profile test,docker,sge
```

It is recommended to run this command using `tmux`, such that you can monitor
the progress of your job even if you logout from the cluster.

## Authors

* Giovanni Stracquadanio, giovanni.stracquadanio@ed.ac.uk
